{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXHWhL+qjBVnNSIn0tllVD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NT7tN70x-wZE","executionInfo":{"status":"ok","timestamp":1770102486289,"user_tz":-360,"elapsed":26486,"user":{"displayName":"Rafi Ahamed","userId":"10687247286440946045"}},"outputId":"634093cc-c3a1-4ae7-d07d-1536179e6f1b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras import datasets, layers, models\n","from tensorflow.keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from collections import Counter\n","\n","\n","(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()"]},{"cell_type":"code","source":["'''\n","  This cell's script is generated from DeepSeek\n","\n","'''\n","# Normalize\n","X_train = X_train.astype('float32') / 255.0\n","X_test = X_test.astype('float32') / 255.0\n","\n","print(f\"Original shapes:\")\n","print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n","print(f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n","\n","# Split parameters\n","num_training = 49000\n","num_validation = 1000\n","num_test = 1000\n","num_dev = 500\n","\n","# Verify we have enough data\n","print(f\"\\nChecking data requirements:\")\n","print(f\"Need {num_training + num_validation} train+val samples, have {len(X_train)}\")\n","print(f\"Need {num_test} test samples, have {len(X_test)}\")\n","\n","if len(X_train) < num_training + num_validation:\n","    print(f\"ERROR: Not enough training data!\")\n","    # Adjust or raise error\n","    num_training = len(X_train) - num_validation\n","    print(f\"Adjusting num_training to {num_training}\")\n","\n","if len(X_test) < num_test:\n","    print(f\"ERROR: Not enough test data!\")\n","    num_test = len(X_test)\n","    print(f\"Adjusting num_test to {num_test}\")\n","\n","# 1. Extract validation set (last 1000 from training)\n","X_val = X_train[num_training:num_training + num_validation]\n","y_val = y_train[num_training:num_training + num_validation]\n","\n","# 2. Update training set (first 49000)\n","X_train = X_train[:num_training]\n","y_train = y_train[:num_training]\n","\n","# 3. Create development set (random 500 from training)\n","np.random.seed(42)\n","dev_indices = np.random.choice(num_training, num_dev, replace=False)\n","X_dev = X_train[dev_indices]\n","y_dev = y_train[dev_indices]\n","\n","# 4. Update test set (first 1000)\n","X_test = X_test[:num_test]\n","y_test = y_test[:num_test]\n","\n","print('\\n' + '='*50)\n","print('FINAL DATA SPLITS:')\n","print('='*50)\n","print(f'X_train shape: {X_train.shape}')\n","print(f'y_train shape: {y_train.shape} -> {y_train.flatten().shape} (flattened)')\n","print(f'X_val shape:   {X_val.shape}')\n","print(f'y_val shape:   {y_val.shape} -> {y_val.flatten().shape} (flattened)')\n","print(f'X_test shape:  {X_test.shape}')\n","print(f'y_test shape:  {y_test.shape} -> {y_test.flatten().shape} (flattened)')\n","print(f'X_dev shape:   {X_dev.shape}')\n","print(f'y_dev shape:   {y_dev.shape} -> {y_dev.flatten().shape} (flattened)')\n","\n","# Verify no overlap\n","print('\\n' + '='*50)\n","print('VERIFICATION:')\n","print('='*50)\n","\n","# Check that all dev indices are within training range\n","print(f\"Dev indices range: [{min(dev_indices)}, {max(dev_indices)}]\")\n","print(f\"Training samples: {num_training}\")\n","print(f\"All dev indices in training range: {max(dev_indices) < num_training}\")\n","\n","# Check unique samples\n","print(f\"\\nUnique samples in each set:\")\n","print(f\"Training: {len(np.unique(dev_indices))} unique dev indices out of {num_dev}\")\n","\n","# Show data statistics\n","print(f\"\\nData Statistics:\")\n","print(f\"Total training samples: {len(X_train)}\")\n","print(f\"Total validation samples: {len(X_val)}\")\n","print(f\"Total test samples: {len(X_test)}\")\n","print(f\"Development samples: {len(X_dev)}\")\n","print(f\"Grand total: {len(X_train) + len(X_val) + len(X_test)}\")\n","\n","# Class distribution\n","print(f\"\\nClass distribution in training set:\")\n","unique, counts = np.unique(y_train, return_counts=True)\n","for cls, count in zip(unique, counts):\n","    print(f\"  Class {cls}: {count} samples ({count/len(y_train)*100:.1f}%)\")\n","\n","# Optional: Create flattened versions for linear classifiers\n","print('\\n' + '='*50)\n","print('FLATTENED VERSIONS (for linear classifiers):')\n","print('='*50)\n","X_train_flat = X_train.reshape(X_train.shape[0], -1)\n","X_val_flat = X_val.reshape(X_val.shape[0], -1)\n","X_test_flat = X_test.reshape(X_test.shape[0], -1)\n","X_dev_flat = X_dev.reshape(X_dev.shape[0], -1)\n","\n","print(f\"X_train_flat: {X_train_flat.shape}\")\n","print(f\"X_val_flat:   {X_val_flat.shape}\")\n","print(f\"X_test_flat:  {X_test_flat.shape}\")\n","print(f\"X_dev_flat:   {X_dev_flat.shape}\")\n","\n","# Save the splits (optional)\n","data_splits = {\n","    'X_train': X_train, 'y_train': y_train.flatten(),\n","    'X_val': X_val, 'y_val': y_val.flatten(),\n","    'X_test': X_test, 'y_test': y_test.flatten(),\n","    'X_dev': X_dev, 'y_dev': y_dev.flatten(),\n","    'X_train_flat': X_train_flat,\n","    'X_val_flat': X_val_flat,\n","    'X_test_flat': X_test_flat,\n","    'X_dev_flat': X_dev_flat,\n","    'dev_indices': dev_indices\n","}\n","\n","print('\\n' + '='*50)\n","print('READY FOR TRAINING!')\n","print('='*50)\n","print(\"You can now use:\")\n","print(\"- X_train, y_train for training\")\n","print(\"- X_val, y_val for validation during training\")\n","print(\"- X_test, y_test for final evaluation\")\n","print(\"- X_dev, y_dev for quick development/debugging\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ihjrkb_-0Tm","executionInfo":{"status":"ok","timestamp":1770102487746,"user_tz":-360,"elapsed":1453,"user":{"displayName":"Rafi Ahamed","userId":"10687247286440946045"}},"outputId":"4706d02d-ccef-4716-808e-4048a5bbe679"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Original shapes:\n","X_train: (50000, 32, 32, 3), y_train: (50000, 1)\n","X_test: (10000, 32, 32, 3), y_test: (10000, 1)\n","\n","Checking data requirements:\n","Need 50000 train+val samples, have 50000\n","Need 1000 test samples, have 10000\n","\n","==================================================\n","FINAL DATA SPLITS:\n","==================================================\n","X_train shape: (49000, 32, 32, 3)\n","y_train shape: (49000, 1) -> (49000,) (flattened)\n","X_val shape:   (1000, 32, 32, 3)\n","y_val shape:   (1000, 1) -> (1000,) (flattened)\n","X_test shape:  (1000, 32, 32, 3)\n","y_test shape:  (1000, 1) -> (1000,) (flattened)\n","X_dev shape:   (500, 32, 32, 3)\n","y_dev shape:   (500, 1) -> (500,) (flattened)\n","\n","==================================================\n","VERIFICATION:\n","==================================================\n","Dev indices range: [4, 48936]\n","Training samples: 49000\n","All dev indices in training range: True\n","\n","Unique samples in each set:\n","Training: 500 unique dev indices out of 500\n","\n","Data Statistics:\n","Total training samples: 49000\n","Total validation samples: 1000\n","Total test samples: 1000\n","Development samples: 500\n","Grand total: 51000\n","\n","Class distribution in training set:\n","  Class 0: 4913 samples (10.0%)\n","  Class 1: 4881 samples (10.0%)\n","  Class 2: 4921 samples (10.0%)\n","  Class 3: 4888 samples (10.0%)\n","  Class 4: 4922 samples (10.0%)\n","  Class 5: 4902 samples (10.0%)\n","  Class 6: 4898 samples (10.0%)\n","  Class 7: 4893 samples (10.0%)\n","  Class 8: 4887 samples (10.0%)\n","  Class 9: 4895 samples (10.0%)\n","\n","==================================================\n","FLATTENED VERSIONS (for linear classifiers):\n","==================================================\n","X_train_flat: (49000, 3072)\n","X_val_flat:   (1000, 3072)\n","X_test_flat:  (1000, 3072)\n","X_dev_flat:   (500, 3072)\n","\n","==================================================\n","READY FOR TRAINING!\n","==================================================\n","You can now use:\n","- X_train, y_train for training\n","- X_val, y_val for validation during training\n","- X_test, y_test for final evaluation\n","- X_dev, y_dev for quick development/debugging\n"]}]},{"cell_type":"code","source":["'''\n","  This cell's script is generated from DeepSeek\n","\n","'''\n","# Extract the flattened data from your splits\n","X_train_flat = data_splits['X_train_flat']\n","y_train = data_splits['y_train']\n","X_val_flat = data_splits['X_val_flat']\n","y_val = data_splits['y_val']\n","X_test_flat = data_splits['X_test_flat']\n","y_test = data_splits['y_test']\n","\n","print(\"Data shapes for training:\")\n","print(f\"X_train_flat: {X_train_flat.shape}\")\n","print(f\"y_train: {y_train.shape} (should be 1D)\")\n","print(f\"X_val_flat: {X_val_flat.shape}\")\n","print(f\"y_val: {y_val.shape} (should be 1D)\")\n","\n","# 2. STANDARDIZE THE DATA (CRITICAL STEP!)\n","# Softmax classifiers need standardized features for gradient descent to work well\n","def standardize_features(X_train, X_val, X_test=None):\n","    \"\"\"Standardize features to have zero mean and unit variance.\"\"\"\n","    # Compute mean and std from training data\n","    mean = np.mean(X_train, axis=0)\n","    std = np.std(X_train, axis=0)\n","    std[std == 0] = 1  # Avoid division by zero\n","\n","    # Standardize all sets using training statistics\n","    X_train_std = (X_train - mean) / std\n","    X_val_std = (X_val - mean) / std\n","\n","    if X_test is not None:\n","        X_test_std = (X_test - mean) / std\n","        return X_train_std, X_val_std, X_test_std\n","\n","    return X_train_std, X_val_std\n","\n","print(\"\\nStandardizing features...\")\n","X_train_std, X_val_std, X_test_std = standardize_features(X_train_flat, X_val_flat, X_test_flat)\n","\n","print(f\"After standardization:\")\n","print(f\"X_train_std - Mean: {np.mean(X_train_std):.6f}, Std: {np.std(X_train_std):.6f}\")\n","print(f\"X_val_std - Mean: {np.mean(X_val_std):.6f}, Std: {np.std(X_val_std):.6f}\")\n","print(f\"X_test_std - Mean: {np.mean(X_test_std):.6f}, Std: {np.std(X_test_std):.6f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WfRngRwUTopO","executionInfo":{"status":"ok","timestamp":1770102489408,"user_tz":-360,"elapsed":1655,"user":{"displayName":"Rafi Ahamed","userId":"10687247286440946045"}},"outputId":"7f62bf29-3011-4623-b9ec-434e326f5015"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Data shapes for training:\n","X_train_flat: (49000, 3072)\n","y_train: (49000,) (should be 1D)\n","X_val_flat: (1000, 3072)\n","y_val: (1000,) (should be 1D)\n","\n","Standardizing features...\n","After standardization:\n","X_train_std - Mean: -0.000000, Std: 1.000003\n","X_val_std - Mean: 0.013165, Std: 0.994525\n","X_test_std - Mean: 0.017657, Std: 0.994975\n"]}]},{"cell_type":"code","source":["class SoftmaxClassifier:\n","    def __init__(self, learning_rate=1e-2, reg_strength=1e-4, num_iters=1000, batch_size=200, verbose=True):\n","        self.learning_rate = learning_rate\n","        self.reg_strength = reg_strength\n","        self.num_iters = num_iters\n","        self.batch_size = batch_size\n","        self.verbose = verbose\n","        self.W = None\n","        self.loss_history = []\n","        self.train_acc_history = []\n","        self.val_acc_history = []\n","\n","    def _softmax(self, scores):\n","        \"\"\"Numerically stable softmax.\"\"\"\n","        # Subtract max for numerical stability\n","        exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n","        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n","\n","    def _loss_grad(self, X, y):\n","        \"\"\"Compute loss and gradient.\"\"\"\n","        num_train = X.shape[0]\n","\n","        # Forward pass\n","        scores = X.dot(self.W)\n","        probs = self._softmax(scores)\n","\n","        # Compute loss\n","        correct_logprobs = -np.log(probs[np.arange(num_train), y] + 1e-8)\n","        data_loss = np.sum(correct_logprobs) / num_train\n","        reg_loss = 0.5 * self.reg_strength * np.sum(self.W * self.W)\n","        loss = data_loss + reg_loss\n","\n","        # Compute gradient\n","        dscores = probs.copy()\n","        dscores[np.arange(num_train), y] -= 1\n","        dscores /= num_train\n","\n","        dW = X.T.dot(dscores) + self.reg_strength * self.W\n","\n","        return loss, dW\n","\n","    def train(self, X_train, y_train, X_val=None, y_val=None):\n","        \"\"\"Train the classifier.\"\"\"\n","        num_train, dim = X_train.shape\n","        num_classes = len(np.unique(y_train))\n","\n","        # Initialize weights with small random values\n","        self.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","        print(f\"Training info:\")\n","        print(f\"  Samples: {num_train}, Features: {dim}, Classes: {num_classes}\")\n","        print(f\"  Learning rate: {self.learning_rate}\")\n","        print(f\"  Batch size: {self.batch_size}\")\n","        print(f\"  Regularization: {self.reg_strength}\")\n","\n","        # Training loop\n","        for it in range(self.num_iters):\n","            # Mini-batch\n","            batch_indices = np.random.choice(num_train, self.batch_size, replace=False)\n","            X_batch = X_train[batch_indices]\n","            y_batch = y_train[batch_indices]\n","\n","            # Compute loss and gradient\n","            loss, dW = self._loss_grad(X_batch, y_batch)\n","            self.loss_history.append(loss)\n","\n","            # Update weights\n","            self.W -= self.learning_rate * dW\n","\n","            # Track accuracy every 100 iterations\n","            if it % 100 == 0 or it < 10:\n","                train_acc = self.score(X_batch, y_batch)\n","                self.train_acc_history.append(train_acc)\n","\n","                if X_val is not None and y_val is not None:\n","                    val_acc = self.score(X_val, y_val)\n","                    self.val_acc_history.append(val_acc)\n","\n","                # Print progress\n","                if self.verbose and it % 100 == 0:\n","                    msg = f\"Iteration {it:4d}/{self.num_iters}: loss = {loss:.4f}, train_acc = {train_acc:.4f}\"\n","                    if X_val is not None and y_val is not None:\n","                        msg += f\", val_acc = {val_acc:.4f}\"\n","                    print(msg)\n","\n","        if self.verbose:\n","            print(f\"Training completed. Final loss: {loss:.4f}\")\n","        return self\n","\n","    def predict(self, X):\n","        \"\"\"Predict labels.\"\"\"\n","        scores = X.dot(self.W)\n","        return np.argmax(scores, axis=1)\n","\n","    def predict_proba(self, X):\n","        \"\"\"Predict probabilities.\"\"\"\n","        scores = X.dot(self.W)\n","        return self._softmax(scores)\n","\n","    def score(self, X, y):\n","        \"\"\"Compute accuracy.\"\"\"\n","        pred = self.predict(X)\n","        return np.mean(pred == y)"],"metadata":{"id":"PjmfZ_oY_KBp","executionInfo":{"status":"ok","timestamp":1770102489415,"user_tz":-360,"elapsed":2,"user":{"displayName":"Rafi Ahamed","userId":"10687247286440946045"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Train the Softmax Classifier\n","print(\"\\n\" + \"=\"*60)\n","print(\"Training Softmax Classifier\")\n","print(\"=\"*60)\n","\n","# Create classifier with good hyperparameters\n","smc = SoftmaxClassifier(\n","    learning_rate=1e-2,      # Good starting point\n","    reg_strength=1e-4,\n","    num_iters=1000,\n","    batch_size=200,\n","    verbose=True\n",")\n","\n","# Train\n","smc.train(X_train_std, y_train, X_val_std, y_val)\n","\n","# Evaluate on all sets\n","print(\"\\n\" + \"=\"*60)\n","print(\"Final Evaluation\")\n","print(\"=\"*60)\n","\n","train_acc = smc.score(X_train_std, y_train)\n","val_acc = smc.score(X_val_std, y_val)\n","test_acc = smc.score(X_test_std, y_test)\n","\n","print(f\"Training accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n","print(f\"Validation accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n","print(f\"Test accuracy:       {test_acc:.4f} ({test_acc*100:.2f}%)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5buSndwQdZ7","executionInfo":{"status":"ok","timestamp":1770102499390,"user_tz":-360,"elapsed":9975,"user":{"displayName":"Rafi Ahamed","userId":"10687247286440946045"}},"outputId":"0dcd2fcd-ec64-4642-cac8-106e0f3c99d2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","Training Softmax Classifier\n","============================================================\n","Training info:\n","  Samples: 49000, Features: 3072, Classes: 10\n","  Learning rate: 0.01\n","  Batch size: 200\n","  Regularization: 0.0001\n","Iteration    0/1000: loss = 2.3068, train_acc = 0.3550, val_acc = 0.2190\n","Iteration  100/1000: loss = 1.7701, train_acc = 0.4650, val_acc = 0.3890\n","Iteration  200/1000: loss = 1.7570, train_acc = 0.4600, val_acc = 0.3960\n","Iteration  300/1000: loss = 1.7461, train_acc = 0.4450, val_acc = 0.3860\n","Iteration  400/1000: loss = 1.8297, train_acc = 0.4200, val_acc = 0.3860\n","Iteration  500/1000: loss = 1.5433, train_acc = 0.5300, val_acc = 0.3860\n","Iteration  600/1000: loss = 1.7174, train_acc = 0.5250, val_acc = 0.3930\n","Iteration  700/1000: loss = 1.8510, train_acc = 0.3850, val_acc = 0.4020\n","Iteration  800/1000: loss = 1.6829, train_acc = 0.4450, val_acc = 0.4040\n","Iteration  900/1000: loss = 1.7462, train_acc = 0.5000, val_acc = 0.4210\n","Training completed. Final loss: 1.6307\n","\n","============================================================\n","Final Evaluation\n","============================================================\n","Training accuracy:   0.4217 (42.17%)\n","Validation accuracy: 0.3960 (39.60%)\n","Test accuracy:       0.3990 (39.90%)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Zp4C82ZASUAW","executionInfo":{"status":"ok","timestamp":1770102499406,"user_tz":-360,"elapsed":11,"user":{"displayName":"Rafi Ahamed","userId":"10687247286440946045"}}},"execution_count":5,"outputs":[]}]}